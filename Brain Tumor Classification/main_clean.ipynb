{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa75b93d",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification - Cleaned Version\n",
    "\n",
    "**Current Best:** v7 (Pseudo-label) = **0.89293**\n",
    "\n",
    "**Top 3 Strategies:**\n",
    "1. v7: Pseudo-labeling (98% confidence) - 0.89293 ‚úÖ\n",
    "2. v16: Neural Network (deep learning) - TBD\n",
    "3. v14: Extreme Ensemble - 0.890000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c299f",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba681884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (7000, 20)\n",
      "Test: (3000, 19)\n",
      "\n",
      "Target distribution:\n",
      "cancer_stage\n",
      "I      0.035714\n",
      "II     0.068714\n",
      "III    0.219143\n",
      "IV     0.676429\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_ids = test_df['id']\n",
    "\n",
    "print(f\"Training: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['cancer_stage'].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c86c85",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering (Proven Features Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8374fee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Apply feature engineering\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m train_df_engineered \u001b[38;5;241m=\u001b[39m engineer_features(train_df)\n\u001b[0;32m     45\u001b[0m test_df_engineered \u001b[38;5;241m=\u001b[39m engineer_features(test_df)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Features engineered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df_engineered\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mengineer_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m100\u001b[39m], labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Ki67 category\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mki67_category\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mki67_index\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m100\u001b[39m], labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Mitotic category\u001b[39;00m\n\u001b[0;32m     22\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmitotic_category\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmitotic_count\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m100\u001b[39m], labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6530\u001b[0m     results \u001b[38;5;241m=\u001b[39m [ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m   6532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6533\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6534\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6535\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    416\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    417\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    418\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    419\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[0;32m    420\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    620\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    235\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:180\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:550\u001b[0m, in \u001b[0;36mCategorical.astype\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert float NaN to integer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    553\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    554\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    555\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    556\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    557\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Add only proven medical domain features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Core aggressiveness score\n",
    "    df['aggressiveness_score'] = df['ki67_index'] * 0.5 + df['mitotic_count'] * 2.5\n",
    "    \n",
    "    # Risk score\n",
    "    df['risk_score'] = (\n",
    "        df['necrosis'] * 3 + \n",
    "        df['hemorrhage'] * 2 + \n",
    "        df['edema'] * 1\n",
    "    )\n",
    "    \n",
    "    # Age group\n",
    "    df['age_group'] = pd.cut(df['age'], bins=[0, 40, 60, 100], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    # Ki67 category\n",
    "    df['ki67_category'] = pd.cut(df['ki67_index'], bins=[0, 10, 20, 100], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    # Mitotic category\n",
    "    df['mitotic_category'] = pd.cut(df['mitotic_count'], bins=[0, 5, 15, 100], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    # Symptoms severity\n",
    "    df['symptoms_severity'] = df['neurological_deficit'] + df['seizures'] + df['headache']\n",
    "    \n",
    "    # KPS category\n",
    "    df['kps_category'] = pd.cut(df['kps_score'], bins=[0, 60, 80, 100], labels=[0, 1, 2]).astype(int)\n",
    "    \n",
    "    # Tumor complexity\n",
    "    df['tumor_complexity'] = (\n",
    "        df['calcification'] + \n",
    "        df['cystic_components'] + \n",
    "        df['necrosis']\n",
    "    )\n",
    "    \n",
    "    # Interactions\n",
    "    df['ki67_mitotic_interaction'] = df['ki67_index'] * df['mitotic_count'] / 100\n",
    "    df['age_ki67_interaction'] = df['age'] * df['ki67_index'] / 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_df_engineered = engineer_features(train_df)\n",
    "test_df_engineered = engineer_features(test_df)\n",
    "\n",
    "print(f\"‚úÖ Features engineered: {train_df_engineered.shape[1]} total columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc67766",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ad002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = train_df_engineered.drop(['cancer_stage'], axis=1)\n",
    "y = train_df_engineered['cancer_stage']\n",
    "\n",
    "X_test = test_df_engineered.drop(['id'], axis=1)\n",
    "\n",
    "# Label encode target\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94463e2a",
   "metadata": {},
   "source": [
    "## 4. Model Training - Base CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best CatBoost parameters (from previous tuning)\n",
    "best_catboost_params = {\n",
    "    'random_strength': 1,\n",
    "    'learning_rate': 0.07,\n",
    "    'l2_leaf_reg': 5,\n",
    "    'iterations': 1000,\n",
    "    'depth': 4,\n",
    "    'border_count': 32,\n",
    "    'bagging_temperature': 0.5,\n",
    "    'random_state': 42,\n",
    "    'verbose': 0,\n",
    "    'task_type': 'CPU'\n",
    "}\n",
    "\n",
    "# Train base model\n",
    "catboost_model = CatBoostClassifier(**best_catboost_params)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "val_pred = catboost_model.predict(X_val)\n",
    "val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "print(f\"CatBoost Validation F1: {val_f1:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9600d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚≠ê STRATEGY #1: Pseudo-Labeling (v7) - BEST: 0.89293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f33860",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY #1: PSEUDO-LABELING (98% CONFIDENCE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train on full training data\n",
    "pseudo_base_model = CatBoostClassifier(**best_catboost_params)\n",
    "pseudo_base_model.fit(X, y_encoded)\n",
    "\n",
    "# Get predictions on test set with confidence\n",
    "test_proba = pseudo_base_model.predict_proba(X_test)\n",
    "test_pred = np.argmax(test_proba, axis=1)\n",
    "test_confidence = np.max(test_proba, axis=1)\n",
    "\n",
    "# Select high-confidence predictions (‚â•98%)\n",
    "confidence_threshold = 0.98\n",
    "high_conf_mask = test_confidence >= confidence_threshold\n",
    "high_conf_indices = np.where(high_conf_mask)[0]\n",
    "\n",
    "print(f\"\\nüìä High-confidence samples at 98%: {len(high_conf_indices)} ({len(high_conf_indices)/len(test_pred)*100:.1f}%)\")\n",
    "\n",
    "if len(high_conf_indices) > 0:\n",
    "    # Get pseudo-labeled samples\n",
    "    X_pseudo = X_test.iloc[high_conf_indices].copy()\n",
    "    y_pseudo = test_pred[high_conf_indices]\n",
    "    \n",
    "    # Combine original + pseudo-labeled data\n",
    "    X_combined = pd.concat([X, X_pseudo], axis=0, ignore_index=True)\n",
    "    y_combined = np.concatenate([y_encoded, y_pseudo])\n",
    "    \n",
    "    print(f\"   Combined training: {len(X_combined)} samples (+{len(X_pseudo)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Retrain model\n",
    "    pseudo_model = CatBoostClassifier(**best_catboost_params)\n",
    "    pseudo_model.fit(X_combined, y_combined)\n",
    "    \n",
    "    # Make final predictions\n",
    "    pseudo_final_pred = pseudo_model.predict(X_test)\n",
    "    pseudo_final_predictions = target_encoder.inverse_transform(pseudo_final_pred)\n",
    "    \n",
    "    # Save submission\n",
    "    submission_v7 = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'cancer_stage': pseudo_final_predictions\n",
    "    })\n",
    "    submission_v7.to_csv('subChromium_v7_pseudo_label.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ v7 Submission created: subChromium_v7_pseudo_label.csv\")\n",
    "    print(f\"üèÜ Kaggle Score: 0.89293 (CURRENT BEST)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No high-confidence predictions found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd1f21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚≠ê STRATEGY #2: Deep Neural Network (v16) - Most Different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d51f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY #2: ADVANCED NEURAL NETWORK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Scale features for neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train deep neural network\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size=64,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nüîÑ Training neural network (3-5 minutes)...\\n\")\n",
    "nn_model.fit(X_scaled, y_encoded)\n",
    "\n",
    "# Make predictions\n",
    "nn_pred = nn_model.predict(X_test_scaled)\n",
    "nn_predictions = target_encoder.inverse_transform(nn_pred)\n",
    "\n",
    "# Save submission\n",
    "submission_v16 = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'cancer_stage': nn_predictions\n",
    "})\n",
    "submission_v16.to_csv('subChromium_v16_neural_network.csv', index=False)\n",
    "\n",
    "# Compare with v7\n",
    "v7_sub = pd.read_csv('subChromium_v7_pseudo_label.csv')\n",
    "differences = (v7_sub['cancer_stage'] != nn_predictions).sum()\n",
    "\n",
    "print(f\"\\n‚úÖ v16 Submission created: subChromium_v16_neural_network.csv\")\n",
    "print(f\"üìä Changes from v7: {differences} predictions ({differences/len(v7_sub)*100:.1f}%)\")\n",
    "print(f\"üéØ Expected: 0.890-0.905 (completely different learning!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f6ae2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚≠ê STRATEGY #3: Extreme Weighted Ensemble (v14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a97c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY #3: EXTREME WEIGHTED ENSEMBLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train additional models for ensemble\n",
    "print(\"\\nüîÑ Training ensemble models...\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='hist'\n",
    ")\n",
    "xgb_model.fit(X, y_encoded)\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(X, y_encoded)\n",
    "\n",
    "# Blending model (from v8)\n",
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "blend_cat = CatBoostClassifier(**best_catboost_params)\n",
    "blend_cat.fit(X_train1, y_train1)\n",
    "\n",
    "blend_xgb = XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.05, random_state=42, eval_metric='mlogloss', tree_method='hist')\n",
    "blend_xgb.fit(X_train1, y_train1)\n",
    "\n",
    "blend_lgb = LGBMClassifier(n_estimators=1000, max_depth=4, learning_rate=0.05, random_state=42, verbose=-1)\n",
    "blend_lgb.fit(X_train1, y_train1)\n",
    "\n",
    "# Train meta-learner\n",
    "train2_cat_proba = blend_cat.predict_proba(X_train2)\n",
    "train2_xgb_proba = blend_xgb.predict_proba(X_train2)\n",
    "train2_lgb_proba = blend_lgb.predict_proba(X_train2)\n",
    "train2_meta = np.hstack([train2_cat_proba, train2_xgb_proba, train2_lgb_proba])\n",
    "\n",
    "blend_meta = LogisticRegression(max_iter=1000, random_state=42, C=0.1)\n",
    "blend_meta.fit(train2_meta, y_train2)\n",
    "\n",
    "# Retrain on full data\n",
    "blend_cat_full = CatBoostClassifier(**best_catboost_params)\n",
    "blend_cat_full.fit(X, y_encoded)\n",
    "\n",
    "blend_xgb_full = XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.05, random_state=42, eval_metric='mlogloss', tree_method='hist')\n",
    "blend_xgb_full.fit(X, y_encoded)\n",
    "\n",
    "blend_lgb_full = LGBMClassifier(n_estimators=1000, max_depth=4, learning_rate=0.05, random_state=42, verbose=-1)\n",
    "blend_lgb_full.fit(X, y_encoded)\n",
    "\n",
    "# Get test probabilities\n",
    "test_cat_proba = blend_cat_full.predict_proba(X_test)\n",
    "test_xgb_proba = blend_xgb_full.predict_proba(X_test)\n",
    "test_lgb_proba = blend_lgb_full.predict_proba(X_test)\n",
    "test_meta_features = np.hstack([test_cat_proba, test_xgb_proba, test_lgb_proba])\n",
    "blend_proba = blend_meta.predict_proba(test_meta_features)\n",
    "\n",
    "# Get v7 probabilities\n",
    "v7_proba = pseudo_model.predict_proba(X_test)\n",
    "\n",
    "# EXTREME weighted ensemble (equal weights)\n",
    "ensemble_proba = (\n",
    "    0.25 * v7_proba +\n",
    "    0.25 * test_cat_proba +\n",
    "    0.25 * test_xgb_proba +\n",
    "    0.20 * blend_proba\n",
    ")\n",
    "\n",
    "ensemble_pred = np.argmax(ensemble_proba, axis=1)\n",
    "ensemble_predictions = target_encoder.inverse_transform(ensemble_pred)\n",
    "\n",
    "# Save submission\n",
    "submission_v14 = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'cancer_stage': ensemble_predictions\n",
    "})\n",
    "submission_v14.to_csv('subChromium_v14_extreme_ensemble.csv', index=False)\n",
    "\n",
    "# Compare with v7\n",
    "differences = (v7_sub['cancer_stage'] != ensemble_predictions).sum()\n",
    "\n",
    "print(f\"\\n‚úÖ v14 Submission created: subChromium_v14_extreme_ensemble.csv\")\n",
    "print(f\"üìä Changes from v7: {differences} predictions ({differences/len(v7_sub)*100:.1f}%)\")\n",
    "print(f\"üèÜ Kaggle Score: 0.890000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cd8ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Summary & Next Steps\n",
    "\n",
    "### **Current Rankings:**\n",
    "\n",
    "| Version | Strategy | Kaggle Score | Status |\n",
    "|---------|----------|--------------|--------|\n",
    "| **v7** | **Pseudo-label (98%)** | **0.89293** | **‚úÖ BEST** |\n",
    "| v16 | Neural Network | TBD | ‚è≥ Submit next! |\n",
    "| v14 | Extreme Ensemble | 0.890000 | ‚úÖ Tested |\n",
    "\n",
    "### **Action Plan:**\n",
    "\n",
    "1. **Submit v16** (Neural Network) - Most different from v7 (10.4% change)\n",
    "2. **If v16 works:** Ensemble v7 + v16 for potential 0.895-0.905\n",
    "3. **If stuck:** Check competition discussions for winning techniques\n",
    "\n",
    "### **Key Learnings:**\n",
    "\n",
    "- ‚úÖ Pseudo-labeling with high confidence (98%) works well\n",
    "- ‚úÖ Neural networks provide different predictions than tree models\n",
    "- ‚úÖ Simple approaches often beat complex ones\n",
    "- ‚ùå More features/data doesn't always help (noise vs signal)\n",
    "- ‚ùå Complex ensembles can overfit\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
